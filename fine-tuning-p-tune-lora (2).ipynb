{"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"widgets":{"application/vnd.jupyter.widget-state+json":{"039b881cc95f4b4c809ec3a527907b42":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bdecd0a1706848b9bea25648021dba5e","placeholder":"​","style":"IPY_MODEL_5367b2a0dd354356b5069107db4bd0ee","value":"100%"}},"094993c5be3d46a690fbc75e4f217adc":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0ccfaf92f4684a219ebb34a6a9108e1d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0e1b1d6328b448ed9b6f45eaf9cd512d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1059549a021240b6aa17e5e83b4a0ba3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"107ed2ed6b4f41b98cbf0a2cd4ed8939":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1106eed2a35c4c268d4c6edf5f85c643":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"164593bc8f9048cabfd3447a40819b05":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"16ddad2721954475bcbad000f44369cb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"1785f765206647c9aae79677b3ef5b86":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1b9b8035f6c64ba6a80aa2105902cf0f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c4c3bfddeee34c93a5ac77f58ad3826c","IPY_MODEL_50fdb22658034563b0d02dd6b6d9cad7","IPY_MODEL_7c82aabb5ecb460096e461e993085245"],"layout":"IPY_MODEL_a86bd2750d044cedacdb710983c49c41"}},"1c632ec373284565af94703bca5f87d8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6540cb7c618b41ec9c81af7833540112","placeholder":"​","style":"IPY_MODEL_107ed2ed6b4f41b98cbf0a2cd4ed8939","value":" 5.55k/5.55k [00:00&lt;00:00, 126kB/s]"}},"1c69bd5349814dd08fe112a00383b0ce":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2565c3d24f524f2ca4c1a3bd40eec3d6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e18d8533c7c24b259ea31ee42c439012","IPY_MODEL_a8f5353d0a2549609d488f818fda04ce","IPY_MODEL_1c632ec373284565af94703bca5f87d8"],"layout":"IPY_MODEL_cc114568a65840fa8c92c6da33c31d95"}},"289364699b0c4117a50b4e23cc8af0a0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2f4ce07967344f12af5d8576e8048cbc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"340bf20db33d4e3498127fb90e278a12":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a25cf303b5fb40a38e92f9043aaab024","max":100,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c04019971c534e8cb3bc780a2ba91bca","value":100}},"398012a6b4ff46f09920684c63eaa4f1":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3c3de4a94fed486084fe97eb5ea04adc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_dca2f770593a4843a2722bfb9e5b02b3","placeholder":"​","style":"IPY_MODEL_c0d375637e5d4622be55558432f3bc23","value":" 100/100 [08:39&lt;00:00,  5.19s/it]"}},"3f91a33f821045639c0b590d749e86f2":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4274987d777349c8b1a98cd0b2c8a321":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"43530a7068d34d43ad768ab883b8cc19":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"46b41a1e94674444aa8c3c871f384374":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4274987d777349c8b1a98cd0b2c8a321","placeholder":"​","style":"IPY_MODEL_90a0d2b5e86141aca5bc64a012085dd0","value":" 647k/647k [00:00&lt;00:00, 5.01MB/s]"}},"4ee4cb3aea224fffa60c048bb74be438":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"50fdb22658034563b0d02dd6b6d9cad7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_6c982241db824e3ba20e806269b680b3","max":2508,"min":0,"orientation":"horizontal","style":"IPY_MODEL_16ddad2721954475bcbad000f44369cb","value":2508}},"524c2277dd664099bb69085e4953202b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5367b2a0dd354356b5069107db4bd0ee":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"547b490dc4ef4337b5ebe633c753bb49":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"56c41973b7914eccad4a94c55af1b05d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e8492250f4924ec3b01bb7dcad72554a","max":33,"min":0,"orientation":"horizontal","style":"IPY_MODEL_681c7c8a482b470eb460fdfbdfe13d4c","value":33}},"5849e2fb31d84d2fa9af5457c2b4297d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"596fbaa9f50040309944b840a2b2a73c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d35bf10e256e4a1ea14339d7f296cbd4","IPY_MODEL_8c36c1599d684f619569b95b2683899b","IPY_MODEL_8b5242c7979a497f8a812e3fb9e6428f"],"layout":"IPY_MODEL_aa5e473e000147b890f72096063dc1f9"}},"59ef4e355eb548c69af3fa43c5cbdee0":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5f4dd9d2746f4035a5755ea7b6a582c8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"6478cee1f71b478c8d970370e1965847":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"6540cb7c618b41ec9c81af7833540112":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6653eb0b9dfd4f47ac7886eeb7a4fabd":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"681c7c8a482b470eb460fdfbdfe13d4c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"68519db1879746bbac4e1ca7fc7cf672":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"6c982241db824e3ba20e806269b680b3":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"729c6712723a4003a30e0720425830fc":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7a256aa44e364199b13ffc9eb27f5706":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a93a881bd8ec4bac998c650ec6861e64","placeholder":"​","style":"IPY_MODEL_524c2277dd664099bb69085e4953202b","value":"quotes.jsonl: 100%"}},"7c82aabb5ecb460096e461e993085245":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a5dbe34fea0d41a6b54d777803345929","placeholder":"​","style":"IPY_MODEL_1c69bd5349814dd08fe112a00383b0ce","value":" 2508/2508 [00:00&lt;00:00, 14879.92 examples/s]"}},"84827bfb51324093909e8df8d5cdfc4a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8b5242c7979a497f8a812e3fb9e6428f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1785f765206647c9aae79677b3ef5b86","placeholder":"​","style":"IPY_MODEL_fe7ecd53ee424559a4b30a7d7555a5c8","value":" 63/100 [00:57&lt;00:31,  1.17it/s]"}},"8c36c1599d684f619569b95b2683899b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"danger","description":"","description_tooltip":null,"layout":"IPY_MODEL_84827bfb51324093909e8df8d5cdfc4a","max":100,"min":0,"orientation":"horizontal","style":"IPY_MODEL_68519db1879746bbac4e1ca7fc7cf672","value":63}},"90a0d2b5e86141aca5bc64a012085dd0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"960e0f1ee6e6413d8895b3d93ba0d2d4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9638b8a54936477693056621d3f9f3c5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9697e55fa1354747a479ff7a86775d68":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"96ae7f28735445d5b336fcd702dcd3f6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b39fa56252ab46b3a24fd3577d2ac449","placeholder":"​","style":"IPY_MODEL_d4e5d34262824731865f84af9e76c84c","value":"Loading checkpoint shards: 100%"}},"9c4d0b83744d42ffab6d2f74ebfc65e8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d286eff453034dc8a15f5e9cb908480a","IPY_MODEL_fd88fbd8bfdf438ba3b6b32fdd1b8202","IPY_MODEL_aaa976169d564f40b2b1471696c06a15"],"layout":"IPY_MODEL_0ccfaf92f4684a219ebb34a6a9108e1d"}},"a25cf303b5fb40a38e92f9043aaab024":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a5dbe34fea0d41a6b54d777803345929":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a86bd2750d044cedacdb710983c49c41":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a8f5353d0a2549609d488f818fda04ce":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a8f8713bfb3f4bd8947de56cd73759b6","max":5554,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6478cee1f71b478c8d970370e1965847","value":5554}},"a8f8713bfb3f4bd8947de56cd73759b6":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a93a881bd8ec4bac998c650ec6861e64":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"aa5e473e000147b890f72096063dc1f9":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"aaa976169d564f40b2b1471696c06a15":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6653eb0b9dfd4f47ac7886eeb7a4fabd","placeholder":"​","style":"IPY_MODEL_960e0f1ee6e6413d8895b3d93ba0d2d4","value":" 32/32 [00:00&lt;00:00, 326.03 examples/s]"}},"b2c019db47f348f082f725ef87589b6c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ed325721bcad4ecd9022909912ad3f38","max":646739,"min":0,"orientation":"horizontal","style":"IPY_MODEL_1106eed2a35c4c268d4c6edf5f85c643","value":646739}},"b39fa56252ab46b3a24fd3577d2ac449":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b52e9d8330844d55bfac3f4972d30bf6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9638b8a54936477693056621d3f9f3c5","placeholder":"​","style":"IPY_MODEL_cb6958905eef4479a7fe6ed170a27ad6","value":"Loading checkpoint shards: 100%"}},"ba8fca4f84114eeb86c18f97239a4895":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_43530a7068d34d43ad768ab883b8cc19","max":33,"min":0,"orientation":"horizontal","style":"IPY_MODEL_5f4dd9d2746f4035a5755ea7b6a582c8","value":33}},"bdecd0a1706848b9bea25648021dba5e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"be2e5e683e08423093da7dcfd7f90411":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c04019971c534e8cb3bc780a2ba91bca":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c0d375637e5d4622be55558432f3bc23":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c177b111ed154ac893fddf3a653da1fd":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_039b881cc95f4b4c809ec3a527907b42","IPY_MODEL_340bf20db33d4e3498127fb90e278a12","IPY_MODEL_3c3de4a94fed486084fe97eb5ea04adc"],"layout":"IPY_MODEL_4ee4cb3aea224fffa60c048bb74be438"}},"c4c3bfddeee34c93a5ac77f58ad3826c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_59ef4e355eb548c69af3fa43c5cbdee0","placeholder":"​","style":"IPY_MODEL_289364699b0c4117a50b4e23cc8af0a0","value":"Generating train split: 100%"}},"c6922c429afa4b73a33a4270eadd36b6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cb6958905eef4479a7fe6ed170a27ad6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cc114568a65840fa8c92c6da33c31d95":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cd0bd07befd945feb799810322473cf3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d286eff453034dc8a15f5e9cb908480a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_094993c5be3d46a690fbc75e4f217adc","placeholder":"​","style":"IPY_MODEL_fa5420a0d29a4b90bb293be5d205a0da","value":"Map: 100%"}},"d35bf10e256e4a1ea14339d7f296cbd4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_547b490dc4ef4337b5ebe633c753bb49","placeholder":"​","style":"IPY_MODEL_cd0bd07befd945feb799810322473cf3","value":" 63%"}},"d4e5d34262824731865f84af9e76c84c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d9b0d2adbb6d4c91847db12e742e15e8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_96ae7f28735445d5b336fcd702dcd3f6","IPY_MODEL_ba8fca4f84114eeb86c18f97239a4895","IPY_MODEL_f911fafe0e3e4efda1b6c286b109f395"],"layout":"IPY_MODEL_729c6712723a4003a30e0720425830fc"}},"dca2f770593a4843a2722bfb9e5b02b3":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dd8167994e5d4d51ab86bef80f7fc866":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7a256aa44e364199b13ffc9eb27f5706","IPY_MODEL_b2c019db47f348f082f725ef87589b6c","IPY_MODEL_46b41a1e94674444aa8c3c871f384374"],"layout":"IPY_MODEL_164593bc8f9048cabfd3447a40819b05"}},"e18d8533c7c24b259ea31ee42c439012":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0e1b1d6328b448ed9b6f45eaf9cd512d","placeholder":"​","style":"IPY_MODEL_2f4ce07967344f12af5d8576e8048cbc","value":"README.md: 100%"}},"e8492250f4924ec3b01bb7dcad72554a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ea39322bcc0848f4a1c2d901c9830f20":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b52e9d8330844d55bfac3f4972d30bf6","IPY_MODEL_56c41973b7914eccad4a94c55af1b05d","IPY_MODEL_f573f939c02d4e42901abea84496b169"],"layout":"IPY_MODEL_be2e5e683e08423093da7dcfd7f90411"}},"ed325721bcad4ecd9022909912ad3f38":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f573f939c02d4e42901abea84496b169":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_398012a6b4ff46f09920684c63eaa4f1","placeholder":"​","style":"IPY_MODEL_c6922c429afa4b73a33a4270eadd36b6","value":" 33/33 [01:24&lt;00:00,  2.66s/it]"}},"f911fafe0e3e4efda1b6c286b109f395":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9697e55fa1354747a479ff7a86775d68","placeholder":"​","style":"IPY_MODEL_1059549a021240b6aa17e5e83b4a0ba3","value":" 33/33 [01:50&lt;00:00,  3.18s/it]"}},"fa5420a0d29a4b90bb293be5d205a0da":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fd88fbd8bfdf438ba3b6b32fdd1b8202":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_3f91a33f821045639c0b590d749e86f2","max":32,"min":0,"orientation":"horizontal","style":"IPY_MODEL_5849e2fb31d84d2fa9af5457c2b4297d","value":32}},"fe7ecd53ee424559a4b30a7d7555a5c8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Методы дообучения Больших Языковых Моделей.\n\n\n**Credits: Данный ноутбук основан на наработках курса NLP от ШАД Яндекса** [yandexdataschool/nlp_course](https://github.com/yandexdataschool/nlp_course)","metadata":{"id":"aSWEcS2XKgzi"}},{"cell_type":"code","source":"%pip install --upgrade transformers accelerate sentencepiece optimum peft bitsandbytes\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport transformers\nfrom tqdm.auto import tqdm, trange\n\nassert torch.cuda.is_available(), \"you need cuda for this part\"\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"id":"7xeRF_hSKgzs","outputId":"a787b22c-23af-4736-e210-77dc20d96684","execution":{"iopub.status.busy":"2024-10-02T15:42:38.798127Z","iopub.execute_input":"2024-10-02T15:42:38.798836Z","iopub.status.idle":"2024-10-02T15:43:19.848118Z","shell.execute_reply.started":"2024-10-02T15:42:38.798791Z","shell.execute_reply":"2024-10-02T15:43:19.846934Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# torch.random.manual_seed(0)\n\n# model = AutoModelForCausalLM.from_pretrained(\n#     \"microsoft/Phi-3.5-mini-instruct\",\n#     device_map=\"cuda\",\n#     torch_dtype=\"auto\",\n#     trust_remote_code=True,\n# )\n# tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3.5-mini-instruct\")\n\n# messages = [\n#     {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},\n#     {\"role\": \"user\", \"content\": \"Can you provide ways to eat combinations of bananas and dragonfruits?\"},\n#     {\"role\": \"assistant\", \"content\": \"Sure! Here are some ways to eat bananas and dragonfruits together: 1. Banana and dragonfruit smoothie: Blend bananas and dragonfruits together with some milk and honey. 2. Banana and dragonfruit salad: Mix sliced bananas and dragonfruits together with some lemon juice and honey.\"},\n#     {\"role\": \"user\", \"content\": \"What about solving an 2x + 3 = 7 equation?\"},\n# ]\n\n# pipe = pipeline(\n#     \"text-generation\",\n#     model=model,\n#     tokenizer=tokenizer,\n# )\n\n# generation_args = {\n#     \"max_new_tokens\": 500,\n#     \"return_full_text\": False,\n#     \"temperature\": 0.0,\n#     \"do_sample\": False,\n# }\n\n# output = pipe(messages, **generation_args)\n# print(output[0]['generated_text'])\n\n# model.gradient_checkpointing_enable()  # only store a small subset of activations, re-compute the rest.\n# model.enable_input_require_grads()  # override an implementation quirk in gradient checkpoints that disables backprop unless inputs require grad","metadata":{"id":"jIWrppVC-2Jl","execution":{"iopub.status.busy":"2024-10-02T15:31:11.572776Z","iopub.execute_input":"2024-10-02T15:31:11.573359Z","iopub.status.idle":"2024-10-02T15:31:25.874187Z","shell.execute_reply.started":"2024-10-02T15:31:11.573313Z","shell.execute_reply":"2024-10-02T15:31:25.873227Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n\nmodel_name = \"Enoch/llama-7b-hf\"\n\n# loading Llama tokenizer ...\ntokenizer = transformers.LlamaTokenizer.from_pretrained(model_name, device_map=device)\ntokenizer.pad_token_id = tokenizer.eos_token_id\n\n# ... and the model itself\nmodel = transformers.AutoModelForCausalLM.from_pretrained(\n    model_name,\n    device_map=\"auto\",\n    low_cpu_mem_usage=True,\n    offload_state_dict=True,\n    load_in_4bit=True,\n    torch_dtype=torch.float32,  # weights are 4-bit; layernorms and activations are fp32\n)\nfor param in model.parameters():\n    param.requires_grad = False\n\nmodel.gradient_checkpointing_enable()  # only store a small subset of activations, re-compute the rest.\nmodel.enable_input_require_grads()  # override an implementation quirk in gradient checkpoints that disables backprop unless inputs require grad\n# more on gradient checkpointing: https://pytorch.org/docs/stable/checkpoint.html https://arxiv.org/abs/1604.06174","metadata":{"id":"VMzFwx29Kgzu","outputId":"9e0fc299-09a1-45db-dba6-b6db33136919","execution":{"iopub.status.busy":"2024-10-02T15:43:19.850307Z","iopub.execute_input":"2024-10-02T15:43:19.853153Z","iopub.status.idle":"2024-10-02T15:46:05.454575Z","shell.execute_reply.started":"2024-10-02T15:43:19.853110Z","shell.execute_reply":"2024-10-02T15:46:05.453685Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Шаг №1: Prompt tuning\n\n![img](https://i.imgur.com/Ux3qQAu.png)\n\nsource: theodd1souts.fandom.com","metadata":{"id":"rgspB2JwSIS2"}},{"cell_type":"code","source":"prompt = \"A quick brown fox\"\nbatch = tokenizer(prompt, return_tensors=\"pt\", return_token_type_ids=False).to(device)\n\nfor i in range(10):\n    next_token = model(**batch).logits[0, -1].argmax(-1).reshape(1, 1)\n    batch[\"input_ids\"] = torch.cat([batch[\"input_ids\"], next_token], dim=-1)\n    batch[\"attention_mask\"] = torch.cat(\n        [batch[\"attention_mask\"], torch.ones_like(next_token)], dim=-1\n    )\n\nprint(\"\\nOutput:\", tokenizer.decode(batch[\"input_ids\"][0].cpu().numpy().tolist()))","metadata":{"id":"H13pYFRxQi4U","outputId":"d19e4eab-7cd9-42de-cf67-b85220d94c57","execution":{"iopub.status.busy":"2024-09-30T05:26:18.504945Z","iopub.execute_input":"2024-09-30T05:26:18.505381Z","iopub.status.idle":"2024-09-30T05:26:20.466986Z","shell.execute_reply.started":"2024-09-30T05:26:18.505344Z","shell.execute_reply":"2024-09-30T05:26:20.466026Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"the_truth = \"A quick brown fox did not jump over the lazy dog. Besides, that dog deserved it anyway!\"\nbatch = tokenizer(the_truth, return_tensors=\"pt\", return_token_type_ids=False).to(\n    device\n)\noutputs = model(**batch)\n\nnext_word_logits = outputs.logits[:, :-1]\nprint(next_word_logits, next_word_logits.shape)\ntrue_next_tokens = batch[\"input_ids\"][:, 1:]\nprint(true_next_tokens, true_next_tokens.shape)\nloss = F.cross_entropy(next_word_logits.flatten(0, 1), true_next_tokens.flatten(0, 1))\n\nprint(\"Loss:\", loss)","metadata":{"id":"_r6UVDl4NEua","outputId":"c1550f82-e099-4a0f-a152-add65ff1e616","execution":{"iopub.status.busy":"2024-09-30T05:26:22.478926Z","iopub.execute_input":"2024-09-30T05:26:22.479917Z","iopub.status.idle":"2024-09-30T05:26:22.794175Z","shell.execute_reply.started":"2024-09-30T05:26:22.479876Z","shell.execute_reply":"2024-09-30T05:26:22.793240Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Воспользуемся механизмом prompt-tuning чтобы модель отвечала \"no dog was jumped over today\" на запросы. Статья о [prompt tuning](https://arxiv.org/abs/2104.08691).\n\n![img](https://i.imgur.com/VwNNKnb.png)\n","metadata":{"id":"amvNufS8WXa0"}},{"cell_type":"code","source":"class WordEmbeddingsWithLearnedPrompts(nn.Module):\n    \"\"\"\n    To perform prompt tuning, you will need to replace model's original word embeddings with a layer - THIS layer\n     - that inserts trainable prompts instead of the first N token embeddings.\"\"\"\n\n    def __init__(self, word_embeddings: nn.Embedding, num_prompts: int):\n        super().__init__()\n        self.original_word_embeddings = word_embeddings\n        self.num_prompts = num_prompts\n        self.learnable_prompts = nn.Parameter(\n            torch.randn(1, num_prompts, word_embeddings.embedding_dim),\n            requires_grad=True,\n        )\n\n    def forward(self, input_ids: torch.LongTensor):\n        # input_ids shape: [batch_size, seq length]\n        assert input_ids.dtype == torch.int64\n        assert input_ids.shape[1] > self.num_prompts\n        assert torch.all(\n            input_ids[:, : self.num_prompts] == tokenizer.pad_token_id\n        ).item(), \"don't forget to prepend several BOS tokens to input_ids\"\n\n        # Your task: embed input_ids, but replace the first :num_prompts: tokens with self.learnable_prompts\n        # This is because we will prepend :num_prompts: padding tokens at the beginning\n\n        # After you are done, you must produce a word embedding vector for each token in input_ids,\n        # except that the first :num_prompts: vectors should equal learnable_prompts;\n        # any additional vectors after first :num_prompts: ones should be embedded as usual\n        # Note: since you're dealing with trainable params, please torch.cat instead of item assignment\n\n        # <YOUR CODE HERE>\n        output = torch.cat(\n            [\n                self.learnable_prompts,\n                self.original_word_embeddings(input_ids[:, self.num_prompts:]),\n            ],\n            dim=1,\n        )\n\n        return output  # your_outputs_with_prompts_as_per_instructions_above","metadata":{"id":"73ZOCFRZWR98","execution":{"iopub.status.busy":"2024-09-30T12:17:36.779722Z","iopub.execute_input":"2024-09-30T12:17:36.780062Z","iopub.status.idle":"2024-09-30T12:17:36.789053Z","shell.execute_reply.started":"2024-09-30T12:17:36.780027Z","shell.execute_reply":"2024-09-30T12:17:36.788074Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_prompts = 16\ntest_emb_layer = WordEmbeddingsWithLearnedPrompts(\n    model.model.embed_tokens, num_prompts=num_prompts\n).to(device)\ntest_input_ids = tokenizer(\"a cat say on a may\", return_tensors=\"pt\")[\"input_ids\"].to(\n    device\n)\n\nspace_for_prompts = torch.full(\n    [len(test_input_ids), num_prompts],\n    fill_value=tokenizer.pad_token_id,\n    dtype=torch.int64,\n    device=device,\n)\ntest_inputs_with_prompts = torch.cat([space_for_prompts, test_input_ids], dim=1)\n\nwith torch.cuda.amp.autocast():\n    test_prompt_embeddings = test_emb_layer(test_inputs_with_prompts)\n\nassert test_prompt_embeddings.shape[:2] == test_inputs_with_prompts.shape\nassert test_prompt_embeddings.shape[-1] == model.config.hidden_size\nassert torch.allclose(\n    test_prompt_embeddings[:, :num_prompts], test_emb_layer.learnable_prompts.float()\n)\nassert torch.allclose(\n    test_prompt_embeddings[:, num_prompts:],\n    model.model.embed_tokens(test_input_ids).float(),\n)\nprint(\"Looks legit!\")","metadata":{"id":"kxUyUU2uT2f1","outputId":"53d2f8c4-d7fe-4e9b-ec24-6e45fb254504","execution":{"iopub.status.busy":"2024-09-30T05:26:40.950450Z","iopub.execute_input":"2024-09-30T05:26:40.950854Z","iopub.status.idle":"2024-09-30T05:26:40.966315Z","shell.execute_reply.started":"2024-09-30T05:26:40.950816Z","shell.execute_reply":"2024-09-30T05:26:40.965401Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"__Работает!__ Давайте посмотрим на результаты.","metadata":{"id":"FbKPgfT-crqW"}},{"cell_type":"code","source":"assert isinstance(\n    model.model.embed_tokens, nn.Embedding\n), \"you have already replaced the embedding layer. If the replacement is broken, please reload the model\"\n\nmodel.model.embed_tokens = WordEmbeddingsWithLearnedPrompts(\n    model.model.embed_tokens, num_prompts=num_prompts\n).to(device)\n\nopt = torch.optim.Adam([model.model.embed_tokens.learnable_prompts], lr=0.01)","metadata":{"id":"QRe0lpREV49G","execution":{"iopub.status.busy":"2024-09-30T05:26:45.461945Z","iopub.execute_input":"2024-09-30T05:26:45.462341Z","iopub.status.idle":"2024-09-30T05:26:45.468969Z","shell.execute_reply.started":"2024-09-30T05:26:45.462303Z","shell.execute_reply":"2024-09-30T05:26:45.468019Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tqdm.auto import tqdm as tqdma","metadata":{"id":"xt-mk4HjudKM","execution":{"iopub.status.busy":"2024-09-30T05:26:47.669068Z","iopub.execute_input":"2024-09-30T05:26:47.669505Z","iopub.status.idle":"2024-09-30T05:26:47.674724Z","shell.execute_reply.started":"2024-09-30T05:26:47.669452Z","shell.execute_reply":"2024-09-30T05:26:47.673550Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"the_truth = \"A quick brown fox did not jump over the lazy dog. Besides, that dog deserved it anyway!\"\nbatch = tokenizer(the_truth, return_tensors=\"pt\", return_token_type_ids=False).to(\n    device\n)\nspace_for_prompts = torch.full(\n    [len(test_input_ids), num_prompts],\n    fill_value=tokenizer.pad_token_id,\n    dtype=torch.int64,\n    device=device,\n)\nbatch[\"input_ids\"] = torch.cat([space_for_prompts, batch[\"input_ids\"]], dim=1)\nbatch[\"attention_mask\"] = torch.cat(\n    [torch.ones_like(space_for_prompts), batch[\"attention_mask\"]], dim=1\n)\n\nfor _ in tqdma(range(100)):\n    outputs = model(**batch)\n    next_word_logits = outputs.logits[:, num_prompts:-1, :]\n    true_next_tokens = batch[\"input_ids\"][:, num_prompts+1:]\n    loss = F.cross_entropy(\n        next_word_logits.flatten(0, 1), true_next_tokens.flatten(0, 1)\n    )\n    print(\"Loss:\", loss)\n    loss.backward()\n    opt.step()\n    opt.zero_grad()\n\n    if loss.item() <= 0.1:\n        break\n# raise NotImplemented(\"Your task: iteratively train the model to reduce loss using prompt optimizer (opt)\")\n\n\nassert loss.item() <= 0.1\nprint(\"Good job!\")","metadata":{"id":"3gVQzgdka-Bm","outputId":"cb464125-d9e5-4ac0-dd43-983a63f95caf","execution":{"iopub.status.busy":"2024-09-30T05:26:49.500352Z","iopub.execute_input":"2024-09-30T05:26:49.500753Z","iopub.status.idle":"2024-09-30T05:27:13.140456Z","shell.execute_reply.started":"2024-09-30T05:26:49.500715Z","shell.execute_reply":"2024-09-30T05:27:13.139451Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Output: <s>A quick brown fox did not jump over the lazy dog. Besides, that dog deserved it\n","metadata":{"id":"bS7wmrypMf0-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prompt = \"A quick brown fox\"\nbatch = tokenizer(prompt, return_tensors=\"pt\", return_token_type_ids=False).to(device)\nbatch[\"input_ids\"] = torch.cat([space_for_prompts, batch[\"input_ids\"]], dim=1)\nbatch[\"attention_mask\"] = torch.cat(\n    [torch.ones_like(space_for_prompts), batch[\"attention_mask\"]], dim=1\n)\n\n\nfor i in range(15):\n    next_token = model(**batch).logits[0, -1].argmax(-1).reshape(1, 1)\n    batch[\"input_ids\"] = torch.cat([batch[\"input_ids\"], next_token], dim=-1)\n    batch[\"attention_mask\"] = torch.cat(\n        [batch[\"attention_mask\"], torch.ones_like(next_token)], dim=-1\n    )\n\nprint(\n    \"\\nOutput:\",\n    tokenizer.decode(batch[\"input_ids\"][0, num_prompts:].cpu().numpy().tolist()),\n)\n\n# if you did everything right, the model will deny that the fox jumped over the lazy dog","metadata":{"id":"F7DkWHD-r1Xo","outputId":"2e35df3f-ac3a-4f0b-d50f-85567203db9b","execution":{"iopub.status.busy":"2024-09-30T05:27:13.142200Z","iopub.execute_input":"2024-09-30T05:27:13.142530Z","iopub.status.idle":"2024-09-30T05:27:15.589126Z","shell.execute_reply.started":"2024-09-30T05:27:13.142472Z","shell.execute_reply":"2024-09-30T05:27:15.588148Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1. Invert the words order","metadata":{"id":"ZxsrGrAkpPIh"}},{"cell_type":"code","source":"# checking if the model can learn. Change max_steps for proper training\nimport datasets\n\ndata = datasets.load_dataset(\"Abirate/english_quotes\", split=\"train[:48]\")  # 48 lines\nNUM_SAMPLES = 48","metadata":{"execution":{"iopub.status.busy":"2024-09-30T13:01:53.677737Z","iopub.execute_input":"2024-09-30T13:01:53.678391Z","iopub.status.idle":"2024-09-30T13:01:56.427828Z","shell.execute_reply.started":"2024-09-30T13:01:53.678344Z","shell.execute_reply":"2024-09-30T13:01:56.426925Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data['quote'][0]","metadata":{"execution":{"iopub.status.busy":"2024-09-30T13:01:56.428931Z","iopub.execute_input":"2024-09-30T13:01:56.429580Z","iopub.status.idle":"2024-09-30T13:01:56.439815Z","shell.execute_reply.started":"2024-09-30T13:01:56.429540Z","shell.execute_reply":"2024-09-30T13:01:56.438870Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = data.map(lambda samples: tokenizer([sample[1 : -2] if i % 2 == 0 else sample[1 : -1] for i, sample in enumerate(samples['quote'])]), batched=True)\nmodel._hf_peft_config_loaded = True  # silence a warning from HF trainer","metadata":{"id":"NXDWOo9EXGW8","execution":{"iopub.status.busy":"2024-09-30T13:01:56.440926Z","iopub.execute_input":"2024-09-30T13:01:56.441184Z","iopub.status.idle":"2024-09-30T13:01:56.514896Z","shell.execute_reply.started":"2024-09-30T13:01:56.441155Z","shell.execute_reply":"2024-09-30T13:01:56.513974Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer.decode(data['input_ids'][0])","metadata":{"execution":{"iopub.status.busy":"2024-09-30T13:01:56.516035Z","iopub.execute_input":"2024-09-30T13:01:56.516303Z","iopub.status.idle":"2024-09-30T13:01:56.526079Z","shell.execute_reply.started":"2024-09-30T13:01:56.516273Z","shell.execute_reply":"2024-09-30T13:01:56.524963Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"start_lens = [len(sample) for sample in data['input_ids']]\nstart_lens","metadata":{"execution":{"iopub.status.busy":"2024-09-30T13:01:56.527336Z","iopub.execute_input":"2024-09-30T13:01:56.527676Z","iopub.status.idle":"2024-09-30T13:01:56.538535Z","shell.execute_reply.started":"2024-09-30T13:01:56.527642Z","shell.execute_reply":"2024-09-30T13:01:56.537575Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_batches = []\n\nfor i in range(NUM_SAMPLES):\n    all_batches.append(tokenizer([data['quote'][i][1 : -2] if i % 2 == 0 else data['quote'][i][1 : -1]],\n                                 return_tensors=\"pt\",\n                                 return_token_type_ids=False).to(\n                                    device\n                                ))","metadata":{"id":"1tuHcKIFpgqN","execution":{"iopub.status.busy":"2024-09-30T13:01:56.539812Z","iopub.execute_input":"2024-09-30T13:01:56.540110Z","iopub.status.idle":"2024-09-30T13:01:56.588389Z","shell.execute_reply.started":"2024-09-30T13:01:56.540078Z","shell.execute_reply":"2024-09-30T13:01:56.587433Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for batch in all_batches:\n    ids_sentence = batch['input_ids']\n    reversed_sentence = torch.flip(batch['input_ids'], dims=(1,))\n\n    batch['input_ids'] = torch.cat([ids_sentence, reversed_sentence], dim=1)\n    batch['attention_mask'] =\\\n        torch.cat([batch['attention_mask'],\n                    batch['attention_mask']], dim=1)","metadata":{"id":"_t5zQvv1pxJt","outputId":"89eede89-597c-423b-ee90-cdbd846a4665","execution":{"iopub.status.busy":"2024-09-30T13:01:56.591270Z","iopub.execute_input":"2024-09-30T13:01:56.591598Z","iopub.status.idle":"2024-09-30T13:01:56.647963Z","shell.execute_reply.started":"2024-09-30T13:01:56.591564Z","shell.execute_reply":"2024-09-30T13:01:56.647130Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tqdm.auto import tqdm as tqdma","metadata":{"execution":{"iopub.status.busy":"2024-09-30T13:01:56.649173Z","iopub.execute_input":"2024-09-30T13:01:56.649585Z","iopub.status.idle":"2024-09-30T13:01:56.654455Z","shell.execute_reply.started":"2024-09-30T13:01:56.649539Z","shell.execute_reply":"2024-09-30T13:01:56.653489Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_prompts = 16\nassert isinstance(\n    model.model.embed_tokens, nn.Embedding\n), \"you have already replaced the embedding layer. If the replacement is broken, please reload the model\"\n\nmodel.model.embed_tokens = WordEmbeddingsWithLearnedPrompts(\n    model.model.embed_tokens, num_prompts=num_prompts\n).to(device)\n\nopt = torch.optim.Adam([model.model.embed_tokens.learnable_prompts], lr=0.01)","metadata":{"execution":{"iopub.status.busy":"2024-09-30T12:21:11.363868Z","iopub.execute_input":"2024-09-30T12:21:11.364539Z","iopub.status.idle":"2024-09-30T12:21:11.376486Z","shell.execute_reply.started":"2024-09-30T12:21:11.364498Z","shell.execute_reply":"2024-09-30T12:21:11.375548Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"space_for_prompts = torch.full(\n    [1, num_prompts],\n    fill_value=tokenizer.pad_token_id,\n    dtype=torch.int64,\n    device=device,\n)\n\nfor batch in all_batches:\n    batch[\"input_ids\"] = torch.cat([space_for_prompts, batch[\"input_ids\"]], dim=1)\n    batch[\"attention_mask\"] = torch.cat(\n        [torch.ones_like(space_for_prompts), batch[\"attention_mask\"]], dim=1\n    )","metadata":{"execution":{"iopub.status.busy":"2024-09-30T12:21:13.568962Z","iopub.execute_input":"2024-09-30T12:21:13.569372Z","iopub.status.idle":"2024-09-30T12:21:13.579498Z","shell.execute_reply.started":"2024-09-30T12:21:13.569333Z","shell.execute_reply":"2024-09-30T12:21:13.578559Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for _ in tqdma(range(100)):\n    mean_loss = 0\n    for batch, start_len in zip(all_batches, start_lens):\n        outputs = model(**batch)\n        next_word_logits = outputs.logits[:, num_prompts+start_len-1:-1, :]\n        true_next_tokens = batch[\"input_ids\"][:, num_prompts+start_len:]\n        loss = F.cross_entropy(\n            next_word_logits.flatten(0, 1), true_next_tokens.flatten(0, 1)\n        )\n        mean_loss += loss.item()\n        loss.backward()\n        opt.step()\n        opt.zero_grad()\n\n    print(\"Loss:\", mean_loss / NUM_SAMPLES)\n    if mean_loss / NUM_SAMPLES <= 0.1:\n        break\n# raise NotImplemented(\"Your task: iteratively train the model to reduce loss using prompt optimizer (opt)\")\n\nprint(\"Good job!\")","metadata":{"id":"xsBpRhY6hhKg","outputId":"7a2bf1ba-bacb-49d2-c448-8239721150df","execution":{"iopub.status.busy":"2024-09-30T12:21:16.689353Z","iopub.execute_input":"2024-09-30T12:21:16.690064Z","iopub.status.idle":"2024-09-30T12:34:03.033355Z","shell.execute_reply.started":"2024-09-30T12:21:16.690023Z","shell.execute_reply":"2024-09-30T12:34:03.032123Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prompt = \"Because winners, they get remembered. \"\nbatch = tokenizer(prompt, return_tensors=\"pt\", return_token_type_ids=False).to(device)\nbatch[\"input_ids\"] = torch.cat([space_for_prompts, batch[\"input_ids\"]], dim=1)\nbatch[\"attention_mask\"] = torch.cat(\n    [torch.ones_like(space_for_prompts), batch[\"attention_mask\"]], dim=1\n)\n\n\nfor i in range(15):\n    next_token = model(**batch).logits[0, -1].argmax(-1).reshape(1, 1)\n    batch[\"input_ids\"] = torch.cat([batch[\"input_ids\"], next_token], dim=-1)\n    batch[\"attention_mask\"] = torch.cat(\n        [batch[\"attention_mask\"], torch.ones_like(next_token)], dim=-1\n    )\n\nprint(\n    \"\\nOutput:\",\n    tokenizer.decode(batch[\"input_ids\"][0, num_prompts:].cpu().numpy().tolist()),\n)\n","metadata":{"id":"v7uZ9eDHrwnc","execution":{"iopub.status.busy":"2024-09-30T12:37:41.012525Z","iopub.execute_input":"2024-09-30T12:37:41.013471Z","iopub.status.idle":"2024-09-30T12:37:43.700596Z","shell.execute_reply.started":"2024-09-30T12:37:41.013427Z","shell.execute_reply":"2024-09-30T12:37:43.699606Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from nltk import tokenize\n# tokenizer = tokenize.WordPunctTokenizer()\n# ' '.join(tokenizer.tokenize(data['quote'][0])[::-1])","metadata":{"id":"LAeSTh60Xj2R","outputId":"9998e7dc-d2ea-4e9a-936b-2e947a3af15a"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Шаг 1.1 (опциональный): HuggingFace PEFT\n\nHuggingFace также предоставил широко применимый инструмент для дообучения: [`peft`](https://huggingface.co/docs/peft/index). Многие современные техники: prompt-tuning, LoRA и другие.\n\n","metadata":{"id":"sEkoFNdlshv_"}},{"cell_type":"code","source":"import peft\n\nassert isinstance(model.model.embed_tokens, nn.Embedding), \"please reload the model\"\n\npeft_config = peft.PromptTuningConfig(\n    task_type=peft.TaskType.CAUSAL_LM, num_virtual_tokens=16\n)\nmodel = peft.get_peft_model(\n    model, peft_config\n) # note: for most peft methods, this line also modifies model in-place\nprint(\n    \"Trainable parameters:\",\n    sum(p.numel() for p in model.parameters() if p.requires_grad),\n)\nprint(\n    \"Total parameters (excluding quantization):\",\n    sum(p.numel() for p in model.parameters()),\n)","metadata":{"id":"mqEEpZm2Q4UC","execution":{"iopub.status.busy":"2024-10-02T15:51:54.848601Z","iopub.execute_input":"2024-10-02T15:51:54.849070Z","iopub.status.idle":"2024-10-02T15:51:54.872227Z","shell.execute_reply.started":"2024-10-02T15:51:54.849029Z","shell.execute_reply":"2024-10-02T15:51:54.871186Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# checking if the model can learn. Change max_steps for proper training\nimport datasets\n\ndata = datasets.load_dataset(\"Abirate/english_quotes\", split=\"train[:48]\")  # 48 lines\nNUM_SAMPLES = 48","metadata":{"execution":{"iopub.status.busy":"2024-10-02T15:51:57.223725Z","iopub.execute_input":"2024-10-02T15:51:57.224116Z","iopub.status.idle":"2024-10-02T15:51:57.923224Z","shell.execute_reply.started":"2024-10-02T15:51:57.224079Z","shell.execute_reply":"2024-10-02T15:51:57.921943Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = data.map(lambda samples: tokenizer([sample[1 : -2] if i % 2 == 0 else sample[1 : -1] for i, sample in enumerate(samples['quote'])]), batched=True)\nmodel._hf_peft_config_loaded = True  # silence a warning from HF trainer\nstart_lens = [len(sample) for sample in data['input_ids']]\nmax_start_len = max(start_lens)","metadata":{"execution":{"iopub.status.busy":"2024-10-02T15:52:05.326512Z","iopub.execute_input":"2024-10-02T15:52:05.326968Z","iopub.status.idle":"2024-10-02T15:52:05.348838Z","shell.execute_reply.started":"2024-10-02T15:52:05.326927Z","shell.execute_reply":"2024-10-02T15:52:05.347883Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import Dataset\n\nclass MyDataset(Dataset):\n    def __init__(self, input_ids, attention_mask):\n        self.input_ids = input_ids\n        self.attention_mask = attention_mask\n\n    def __len__(self):\n        return len(self.input_ids)\n\n    def __getitem__(self, idx):\n        input_ids = self.input_ids[idx]\n        attention_mask = self.attention_mask[idx]\n\n        # Labels are typically the input_ids shifted for next-token prediction\n        labels = input_ids.clone()\n\n        return {\n            \"input_ids\": input_ids,\n            \"attention_mask\": attention_mask,\n            \"labels\": labels  # Include labels for loss computation\n        }","metadata":{"execution":{"iopub.status.busy":"2024-10-02T15:59:29.950144Z","iopub.execute_input":"2024-10-02T15:59:29.950644Z","iopub.status.idle":"2024-10-02T15:59:29.960482Z","shell.execute_reply.started":"2024-10-02T15:59:29.950605Z","shell.execute_reply":"2024-10-02T15:59:29.959029Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_batches = {\n    'input_ids': tokenizer.pad_token_id * torch.ones(size=[NUM_SAMPLES, 2 * max_start_len], dtype=torch.int64),\n    'attention_mask': torch.zeros(size=[NUM_SAMPLES, 2 * max_start_len], dtype=torch.int64)\n}\n\nfor i in range(NUM_SAMPLES):\n    pretokenized = tokenizer(data['quote'][i][1 : -2] if i % 2 == 0 else data['quote'][i][1 : -1],\n                                 return_tensors=\"pt\",\n                                 return_token_type_ids=False)\n    \n    ids_sentence = pretokenized['input_ids']\n    reversed_sentence = torch.flip(ids_sentence, dims=(1,))\n    all_batches['input_ids'][i][: 2*start_lens[i]] = torch.cat([ids_sentence, reversed_sentence], dim=1).flatten()\n    all_batches['attention_mask'][0: 2 * len(ids_sentence)] = 1\n    \ntrain_dataset = MyDataset(input_ids=all_batches['input_ids'], attention_mask=all_batches['attention_mask'])","metadata":{"execution":{"iopub.status.busy":"2024-10-02T15:59:32.120823Z","iopub.execute_input":"2024-10-02T15:59:32.121274Z","iopub.status.idle":"2024-10-02T15:59:32.175497Z","shell.execute_reply.started":"2024-10-02T15:59:32.121235Z","shell.execute_reply":"2024-10-02T15:59:32.174614Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import Trainer, TrainingArguments\n\n# Initialize training arguments\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",         # Directory to save results\n    evaluation_strategy=\"no\",       # No evaluation in this example (change if needed)\n    per_device_train_batch_size=4,  # Adjust according to your resources\n    num_train_epochs=20,             # Number of training epochs\n    logging_dir=\"./logs\",           # Logging directory\n    logging_steps=10,               # Log every 10 steps\n    save_strategy=\"epoch\",          # Save model every epoch\n    learning_rate=0.01,             # Learning rate\n)\n\n\n# Initialize the Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset\n)\n\n# Start training\ntrainer.train()\n","metadata":{"execution":{"iopub.status.busy":"2024-10-02T16:00:08.390067Z","iopub.execute_input":"2024-10-02T16:00:08.390502Z","iopub.status.idle":"2024-10-02T17:49:03.931155Z","shell.execute_reply.started":"2024-10-02T16:00:08.390460Z","shell.execute_reply":"2024-10-02T17:49:03.930035Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Encode a prompt\ninput_prompt = \"Once upon a time\"\ninputs = tokenizer(input_prompt, return_tensors=\"pt\").to(device)\n\n# Generate output\noutput = model.generate(**inputs, max_length=50)\ngenerated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n\nprint(generated_text)","metadata":{"execution":{"iopub.status.busy":"2024-10-02T18:10:48.012550Z","iopub.execute_input":"2024-10-02T18:10:48.013722Z","iopub.status.idle":"2024-10-02T18:10:48.373450Z","shell.execute_reply.started":"2024-10-02T18:10:48.013651Z","shell.execute_reply":"2024-10-02T18:10:48.370565Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Your task: optimize the PEFT-wrapped model to achieve next token prediction loss < 0.1, but this time using PEFT\n# Please note: you no longer need to prepend PAD tokens, but you still need to skip :num_virtual_tokens: first logits.\n# Finally, generate the sentence to make sure that the model learned the truth.","metadata":{"id":"UW54GnzCwVpp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Feel free to structure your code as you see fit - as long as it's legible :)","metadata":{"id":"71vJ9Mq7w67f"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Шаг 2: LoRA\n\nПри дообучении для более серьезных задач можно обратиться к линейной алгебре и вспомнить о __ранге матрицы__. Низкоранговые адаптеры на основе матричного разложения описаны в [статье о LoRA](https://arxiv.org/pdf/2106.09685.pdf).\n\nОсновная идея заключается в добавлении низкоранговых адаптеров параллельно с существующими линейными слоями:\n<center><img src=\"https://i.imgur.com/6bQLNiG.png\" width=240px></center>\n\nВ оригинальной статье по LoRA адаптеры добавлялись только к матрицам внимания. Тем не менее, [новые работы](https://arxiv.org/abs/2305.14314) показывают, что также полезно применять их и к полносвязным частям.\n\nДля начала реализуем базовый слой LoRA.","metadata":{"id":"uCkpKYjWxfhk"}},{"cell_type":"code","source":"# re-load the model to remove any previous PEFT tuners\nmodel = transformers.AutoModelForCausalLM.from_pretrained(\n    model_name,\n    device_map=\"auto\",\n    low_cpu_mem_usage=True,\n    offload_state_dict=True,\n    load_in_4bit=True,\n    torch_dtype=torch.float32,  # weights are 4-bit; layernorms and activations are fp32\n)\nfor param in model.parameters():\n    param.requires_grad = False\nmodel.gradient_checkpointing_enable()\nmodel.enable_input_require_grads()","metadata":{"id":"8zundaSzx90r","outputId":"f51dd070-892d-4054-c312-613a5f82973d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class LoRALayer(nn.Module):\n    \"\"\"Wraps a linear layer with LoRA-like adapter. Wraps an existing OPT linear layer\"\"\"\n\n    def __init__(self, module: nn.Linear, rank: int):\n        super().__init__()\n        self.module = module  # pre-trained (frozen) linear layer\n        self.adapter_A = nn.Parameter(\n            torch.empty(module.in_features, rank, device=module.weight.device)\n        )\n        nn.init.kaiming_uniform_(self.adapter_A, a=5**0.5)\n        self.adapter_B = nn.Parameter(\n            torch.zeros(rank, module.out_features, device=module.weight.device)\n        )\n\n    def forward(self, input):\n        # Apply self.module and LoRA adapter, return the sum (self.module outputs + adapter outputs)\n        #  <YOUR CODE HERE>\n        return self.module(input) + torch.matmul(\n            torch.matmul(input, self.adapter_A), self.adapter_B\n        )","metadata":{"id":"MJ_hq4fwyPVR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test your implementation\ntest_linear = nn.Linear(128, 128)\ntest_linear.weight.data[...] = torch.eye(128)\ntest_adapter = LoRALayer(test_linear, rank=8)\n\nassert torch.allclose(\n    test_adapter(torch.ones(1, 1, 128)), test_linear.bias + 1\n), \"please check your forward pass\"\n\ntest_adapter.adapter_A.data[...] = torch.linspace(0.1, -0.5, 128 * 8).view(128, 8)\ntest_adapter.adapter_B.data[...] = torch.linspace(0.5, -0.1, 128 * 8).view(8, 128)\ntest_linear.bias.data[...] = torch.linspace(1.0, -1.0, 128)\n\ndummy_loss = F.mse_loss(\n    test_adapter(torch.ones(1, 128) / 128).squeeze(), torch.linspace(-1, 1, 128)\n)\nassert torch.allclose(dummy_loss, torch.tensor(1.3711389), rtol=0, atol=1e-4)\ndummy_loss.backward()\nassert all(\n    w.grad is not None for w in [test_adapter.adapter_A, test_adapter.adapter_B]\n), \"some adapter weights have no grad\"\nassert torch.allclose(\n    test_adapter.adapter_A.grad.sum(), torch.tensor(-0.60158), rtol=0, atol=1e-4\n), \"bad grad w.r.t. A\"\nassert torch.allclose(\n    test_adapter.adapter_B.grad.sum(), torch.tensor(0.9931), rtol=0, atol=1e-4\n), \"bad grad w.r.t. B\"\n# note: bad grad means that your code is different from LoRA paper OR that your code is not autograd-friendly (e.g. no_grad)\ndel dummy_loss, test_linear, test_adapter\nprint(\"All tests passed!\")","metadata":{"id":"tTzOs65JydcS","outputId":"591920b4-e018-474a-ea17-8ffb5ea331d7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Ниже приведен код, который применяет адаптер LoRA к линейным слоям Q/K/V внимания модели. Модифицировать можно и другие слои:\n* self_attn.o_proj\n* mlp.up_proj, mlp.gate_proj, mlp.down_proj\n* lm_head","metadata":{"id":"tajVTsvLulB6"}},{"cell_type":"code","source":"lora_rank = 8\n\nfor name, module in model.model.layers.named_modules():\n    if \"LlamaDecoderLayer\" in repr(type(module)):\n        module.self_attn.q_proj = LoRALayer(module.self_attn.q_proj, rank=lora_rank).to(\n            device\n        )\n        module.self_attn.k_proj = LoRALayer(module.self_attn.k_proj, rank=lora_rank).to(\n            device\n        )\n        module.self_attn.v_proj = LoRALayer(module.self_attn.v_proj, rank=lora_rank).to(\n            device\n        )\n\nassert (\n    sum(isinstance(module, LoRALayer) for module in model.modules()) == 96\n)  # for Llama-7B","metadata":{"id":"davyUVEwulB6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch = tokenizer(\n    \"This model wants to share its greatest secret:\",\n    return_tensors=\"pt\",\n    return_token_type_ids=False,\n)\n# test a single training step, make sure we get meaningful gradients\nwith torch.cuda.amp.autocast(dtype=torch.float32):\n    out = model.forward(**batch)\n    (out.logits.norm() / 100).backward()\n\nfor i, module in enumerate(model.modules()):\n    if isinstance(module, LoRALayer):\n        assert module.adapter_B.grad is not None\n        assert module.adapter_B.grad.norm().item() > 0\n\nmodel.zero_grad(set_to_none=True)\nprint(\"Grad check successful, well done!\")","metadata":{"id":"AWzfvc0EulB6","outputId":"ab0f3c2d-002b-4176-cdd1-fd81c6f6a546","tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Приведенный ниже пример показывает, как обучить адаптеры LoRA на небольшом наборе данных.","metadata":{"id":"rjIJ1vkUulB7"}},{"cell_type":"code","source":"# checking if the model can learn. Change max_steps for proper training\nimport datasets\n\ndata = datasets.load_dataset(\"Abirate/english_quotes\", split=\"train[:32]\")  # 32 lines\ndata = data.map(lambda samples: tokenizer(samples[\"quote\"]), batched=True)\nmodel._hf_peft_config_loaded = True  # silence a warning from HF trainer","metadata":{"id":"r9mIpntHulB8","outputId":"306393cb-86cf-452d-b820-ab46c2065fdc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"PEFT\nParameter\nEfficient\nFine\nTunin","metadata":{"id":"p_JnN0mIRcmI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer = transformers.Trainer(\n    model=model,\n    train_dataset=data,\n    args=transformers.TrainingArguments(\n        per_device_train_batch_size=2,\n        gradient_accumulation_steps=1,\n        # note: if you want larger batch size, increase gradient_accumulation_steps\n        warmup_steps=250,\n        max_steps=100,\n        learning_rate=2e-4,\n        fp16=True,\n        logging_steps=1,\n        output_dir=\"outputs\",\n        report_to=None,\n    ),\n    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n)\n# if you see cache warnings, set `model.config.use_cache = False` to silence them. Please re-enable for inference!\n\ntrainer.train()\n\n# NOTE: this is just an example! you do not have to wait for this progressbar to finish :)","metadata":{"id":"XBOfIaUSQIVV","outputId":"41a8bfca-f467-44e7-88cd-ae72a181d335"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prompt = \"Если где-то тебе не рады в рваных носках \"\nbatch = tokenizer(prompt, return_tensors=\"pt\", return_token_type_ids=False).to(device)\nfor i in range(15):\n    next_token = model(**batch).logits[0, -1].argmax(-1).reshape(1, 1)\n    batch[\"input_ids\"] = torch.cat([batch[\"input_ids\"], next_token], dim=-1)\n    batch[\"attention_mask\"] = torch.cat(\n        [batch[\"attention_mask\"], torch.ones_like(next_token)], dim=-1\n    )\n\nprint(\n    \"\\nOutput:\",\n    tokenizer.decode(batch[\"input_ids\"][0, :].cpu().numpy().tolist()),\n)\n\n# if you did everything right, the model will deny that the fox jumped over the lazy dog","metadata":{"id":"GIHTQGdZR1sD","outputId":"fbf03009-c047-486d-f543-11f0f7861fd0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"1. Развернуть фразу на входе (на уровне слов) с помощью p-tune\n2. Сделать то же самое с помощью библиотеки peft от HF\n3. Дообучить с помощью LoRA одну из моделей (лучше gemma:2b или phi3.5, т.к. они небольшие) делать что-то прикольное на ваш выбор\n","metadata":{"id":"lbELN_ofSsOJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Шаг 3: Дополнительное задание, *фактическое* обучение модели\n\nВаша задача - дообучить модель для _генерации кода на Python_. Пожалуйста, используйте вышеприведенные примеры в качестве вдохновения. Например:\n\n* dataset: используйте [codeparrot-clean](https://huggingface.co/datasets/codeparrot/codeparrot-clean) или любые другие данные, содержащие код на Python. Так как вам не нужно много данных для этого упражнения, достаточно использовать только более короткий набор данных для валидации codeparrots.\n* предобработка: выберите код на Python на основе расширений файлов (.py) (можно пропустить в случае codeparrot - 100% этого датасета – Python)\n* короткие строки: используйте первые 512 символов каждой строки\n* тип адаптера: используйте LoRA, плюс как минимум один из:\n   - дополнительный адаптер на lm_head\n   - дополнительный адаптер на компоненты MLP (mlp.*)\n   - обучаемые входные эмбеддинги (требуется настройка использования памяти)\n\n* обучение: вам не обязательно обучать до сходимости. Если все пройдет хорошо, ваша модель должна начать генерировать код после 500 шагов. Пожалуйста, используйте batch size не менее 4 (4 x 1 x 512 токенов) с использованием gradient_accumulation_steps=4.\n\nПримечание: в библиотеке peft также есть реализация LoRA. Однако мы просим вас показать хотя бы один полный запуск обучения с вашим собственным кодом LoRA для этого задания.\n\nАльтернативное задание: Вместо написания кода на Python, вы можете заменить задачу любым другим набором данных, например, вашим любимым исполнителем или подкастом, при условии, что это этично. Если вы выберете собственную задачу, пожалуйста, покажите примеры того, что ваша модель выучила - или не выучила, аналогично приведенным ниже примерам кода.","metadata":{"id":"DQUlqoEAulB8"}},{"cell_type":"code","source":"prompts = [\n    \"\",\n    \"import\",\n    \"from\",\n    \"while\",\n    \"try\",\n    \"if\",\n    \"for\",\n    \"torch\",\n]  # feel free to add a few more that are not 100% assiciated with Python\n\n# <A WHOLE LOT OF YOUR CODE>\n# generate baseline samples with the selected prompts before finetuning\n# please feel free to use transformers.Trainer (as above) or your custom training code\n# after the training concludes, please show examples of text generated by your model. It is expected to look like Python code fragments\n# print the generation examples nicely (suggestion: use pandas or HTML) for easier comparison\n# note: your LoRA-enhanced model can run generation the same way as the non-trained model (above)","metadata":{"id":"_LfFWSYhulB8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This template helps to compare generated code samples in pretty table form\n# feel free to present your work in other forms\n\nfrom IPython.display import HTML, display\n\ntable_template = \"\"\"<table style=\"border:1px solid black\" >\n  <tr>\n    <th style=\"text-align: center; border:1px solid black\">PROMPT</th>\n    <th style=\"text-align: center; border:1px solid black\">BEFORE</th>\n    <th style=\"text-align: center; border:1px solid black\">AFTER</th>\n  </tr>\n{}\n</table>\"\"\"\n\nrow_template = \"\"\"  <tr>\n    <td style=\"width:20%; border:1px solid black\"><pre align=\"left\">`{}`</pre></td>\n    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">{}</pre></td>\n    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">{}</pre></td>\n  </tr>\"\"\"\n\nrows = []\n\nfor prompt in prompts:\n    # replace placeholders in the format() arguments\n    rows.append(\n        row_template.format(\n            prompt, \"BEFORE FINETUNING\", \"TO BE GENERATED AFTER FINETUNING\"\n        )\n    )\n\ndisplay(HTML(table_template.format(\"\\n\".join(rows))))","metadata":{"id":"SSucUeB4ulB9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Доп. материалы:\n\n* How post-training quantization works: https://arxiv.org/abs/2208.07339\n* An overview of running large models: https://huggingface.co/docs/accelerate/package_reference/big_modeling\n* A general library for different adapter types: https://adapterhub.ml/\n\n\n### P.s.\nПриведенный выше код можно достаточно легко адаптировать ко многим современным и не очень моделям: [Falcon-7B](https://huggingface.co/tiiuae/falcon-7b), [OPT-6.7B](https://huggingface.co/facebook/opt-6.7b) or [BLOOM-7.1B](https://huggingface.co/bigscience/bloom-7b1).\n\nНо вам может понадобиться изменить некоторые переменные:\n1. Название модели для `AutoModelForCausalLM.from_pretrained()` и `AutoTokenizer`\n2. Для prompt-tuning обратите внимание на `model.model.embed_tokens`.\n3. Доработайте код для добавления LoRA. Сам адаптер не требует изменений.","metadata":{"id":"hrKidv5KulB9"}}]}